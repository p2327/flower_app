{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Build a classifier with Tensorflow\n",
    "---\n",
    "Setup Tensorflow backend to classify *Iris genus* plants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import (absolute_import, \n",
    "                        division, \n",
    "                        print_function,\n",
    "                        unicode_literals)\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\\n\\\n",
    "Eager execution: {tf.executing_eagerly()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def pack_features_vector(features, labels):\n",
    "    \"\"\"\n",
    "    Pack the features into a single array.\n",
    "    Takes values from a list of tensors and creates\n",
    "    combined tensor at the specified dimension.\n",
    "    \"\"\"\n",
    "    features = tf.stack(list(features.values()), axis=1)\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def loss(model, x, y):\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True)\n",
    "    # predictions\n",
    "    y_ = model(x)\n",
    "    # return cross-entropy (actual and predicted)\n",
    "    return loss_object(y_true=y, y_pred=y_)\n",
    "\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # internal call to loss\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    # returns loss and gradients\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Iris genus* entails about 300 species,\n",
    "but our program will only classify the following three:\n",
    " - *Iris setosa*\n",
    " - *Iris virginica*\n",
    " - *Iris versicolor*  \n",
    "\n",
    "Load train and test set and look at the first rows of data with pandas:\n",
    "\n",
    "```python\n",
    "train_dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\"\n",
    "test_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "train_dataset_fp = tf.keras.utils.get_file(\n",
    "    fname=os.path.basename(train_dataset_url),\n",
    "    origin=train_dataset_url\n",
    "    )\n",
    "test_fp = tf.keras.utils.get_file(\n",
    "    fname=os.path.basename(test_url),\n",
    "    origin=test_url\n",
    "    )\n",
    "\n",
    "column_names = ['sepal_length',\n",
    "                'sepal_width',\n",
    "                'petal_length',\n",
    "                'petal_width',\n",
    "                'species']\n",
    "\n",
    "train_df = pd.read_csv(train_dataset_fp,\n",
    "                       names=column_names,\n",
    "                       header=0)\n",
    "train_df.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Iris genus* entails about 300 species,\n",
    "but our program will only classify the following three:\n",
    " - *Iris setosa*\n",
    " - *Iris virginica*\n",
    " - *Iris versicolor*  \n",
    "\n",
    "Load train and test set and look at the first rows of data with pandas:\n",
    "\n",
    "```python\n",
    "train_dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\"\n",
    "test_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "train_dataset_fp = tf.keras.utils.get_file(\n",
    "    fname=os.path.basename(train_dataset_url),\n",
    "    origin=train_dataset_url\n",
    "    )\n",
    "test_fp = tf.keras.utils.get_file(\n",
    "    fname=os.path.basename(test_url),\n",
    "    origin=test_url\n",
    "    )\n",
    "\n",
    "column_names = ['sepal_length',\n",
    "                'sepal_width',\n",
    "                'petal_length',\n",
    "                'petal_width',\n",
    "                'species']\n",
    "\n",
    "train_df = pd.read_csv(train_dataset_fp,\n",
    "                       names=column_names,\n",
    "                       header=0)\n",
    "train_df.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\"\n",
    "test_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "# Define filepaths for train and test set\n",
    "train_dataset_fp = tf.keras.utils.get_file(\n",
    "    fname=os.path.basename(train_dataset_url),\n",
    "    origin=train_dataset_url\n",
    "    )\n",
    "\n",
    "test_fp = tf.keras.utils.get_file(\n",
    "    fname=os.path.basename(test_url),\n",
    "    origin=test_url\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first 5 lines of the dataset\n",
    "with open(train_dataset_fp) as f:\n",
    "    for x in range(5):\n",
    "        head = next(f).strip()\n",
    "        print(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns order in the csv file\n",
    "column_names = ['sepal_length',\n",
    "                'sepal_width',\n",
    "                'petal_length',\n",
    "                'petal_width',\n",
    "                'species']\n",
    "\n",
    "# Load data in a Pandas DataFrame\n",
    "train_df = pd.read_csv(train_dataset_fp,\n",
    "                       names=column_names,\n",
    "                       header=0)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each label is associated with a string name (for example, *setosa*),\n",
    "but machine learning typically relies on numeric values\n",
    "(**0, 1, 2** in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Iris setosa', 'Iris versicolor', 'Iris virginica']\n",
    "\n",
    "# Number of examples in a 'batch' (one iteration of model training)\n",
    "batch_size = 32\n",
    "feature_names = column_names[:-1]\n",
    "label_name = column_names[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Prepare data for TensorFlow\n",
    "Calling ```make_csv_dataset``` to create a **tf.data.Dataset**.\n",
    "\n",
    "The ```make_csv_dataset``` function returns a **tf.data.Dataset** of (features, label) pairs, where ```features``` is a dictionary: ```{'feature_name': value}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    train_dataset_fp,\n",
    "    batch_size,\n",
    "    column_names=column_names,\n",
    "    label_name=label_name,\n",
    "    num_epochs=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Dataset``` objects are iterable.\n",
    "Like-features are grouped together or *batched* and each example row fields are appended to the corresponding feature array.  \n",
    "Changing ```batch_size``` in ```make_csv_dataset``` sets the number of examples stored in these feature arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the next item in the Dataset object\n",
    "features, labels = next(iter(train_dataset))\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore data with a scatter plot. \n",
    "Some clusters are visible by plotting a few features from the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(features['petal_length'],\n",
    "            features['sepal_length'],\n",
    "            c=labels,\n",
    "            cmap='viridis')\n",
    "\n",
    "plt.xlabel(\"Petal length\")\n",
    "plt.ylabel(\"Sepal length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pack the features in a single arrays with shape ```(batch_size, num_features)``` using ```pack_features_vector``` helper function, then use ```tf.data.Dataset.map``` to pack the features of each\n",
    "```(features, label)```pair into the training dataset\n",
    "(it applies the function to each pair).\n",
    "\n",
    "The features element of the ```Dataset``` are now arrays with shape\n",
    "```(batch_size, num_features)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(pack_features_vector)\n",
    "\n",
    "features, labels = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the first 5 examples (i.e. samples)\n",
    "print(features[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The target labels for each example in the batch\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Select the model\n",
    "\n",
    "A model is a relationship between the features and the label.\n",
    "\n",
    "We'll use a neural network to solve the Iris classification problem.   \n",
    "Neural networks can find complex relationships between features and the label. \n",
    "\n",
    "It is a highly-structured graph, organized into one or more hidden layers. \n",
    "Each hidden layer consists of one or more neurons. \n",
    "There are several categories of neural networks and this program uses a dense, \n",
    "or fully-connected neural network: the neurons in one layer receive\n",
    "input connections from every neuron in the previous layer.  \n",
    "\n",
    "When the model from is trained and fed an unlabeled example, \n",
    "it yields three predictions: the likelihood that this flower is \n",
    "the given Iris species. This prediction is called **inference**.\n",
    "\n",
    "<img src='https://www.tensorflow.org/images/custom_estimators/full_network.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Create a model using Keras\n",
    "\n",
    "The ```tf.keras.Sequential``` model is a linear stack of layers.  \n",
    "Its constructor takes a list of layer instances, in this case, two Dense layers with 10 nodes each,\n",
    "and an output layer with 3 nodes representing our label predictions.  \n",
    "\n",
    "The first layer's ```input_shape``` parameter corresponds to the number of features from the dataset, \n",
    "and is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  # Input shape required\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  \n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(3)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation function determines the output shape of each node in the layer. \n",
    "The ideal number of hidden layers and neurons depends on the problem and the dataset.\n",
    "> **Rule of thumb**. Increasing the number of hidden layers and neurons creates a more powerful model, \n",
    "> which requires more data to train effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the model\n",
    "Passing ```features``` in the model calculates predictions for each\n",
    "example in the batch (in our case we have 32 examples per batch).  \n",
    "For each example we obtain a logit for each class in the target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(features)\n",
    "predictions[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ```softmax``` converts the logit to the predicted probability of \n",
    "each example in the batch to be of one of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.softmax(predictions[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling ```tf.argmax``` on ```predictions``` gives the predicted class index. As the model has not been trained yet, these won't be very good just now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Predictions: {tf.argmax(predictions, axis=1)[:5]}\")\n",
    "print(f\"Actual label: {labels[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Train the model\n",
    "#### Define the loss and gradient function\n",
    "Both training and evaluation stages need to calculate the model's loss.\n",
    "This measures how off a model's predictions are from the desired label, or \n",
    "in other words, how bad the model is performing.\n",
    "We want to minimize, or optimize, this value. \n",
    "\n",
    "Our model will calculate its loss using ```tf.keras.losses.SparseCategoricalCrossentropy```\n",
    "which takes the model's class probability predictions and the desired label, and returns the average loss across the examples.\n",
    "\n",
    "```python\n",
    "def loss(model, x, y):\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True)\n",
    "    # predictions\n",
    "    y_ = model(x)\n",
    "    # return cross-entropy (actual and predicted)\n",
    "    return loss_object(y_true=y, y_pred=y_)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the loss helper function\n",
    "crossentropy_loss = loss(model, features, labels)\n",
    "print(f\"Loss test: {crossentropy_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```tf.GradientTape``` calculates gradients used to optimise the model.\n",
    "\n",
    "```python\n",
    "def grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # internal call to loss\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    # returns loss and gradients\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an optimiser\n",
    "\n",
    "An *optimizer* applies the computed gradients to the model's variables\n",
    "to minimize the ```loss``` function.\n",
    "\n",
    "> ##### Gradient descent.\n",
    "The loss function can be thinked of as a curved surface.\n",
    "We want to find its lowest point by walking around.\n",
    "\n",
    "TensorFlow ```tf.train.GradientDescentOptimizer```\n",
    "implements the stochastic gradient descent (SGD) algorithm.\n",
    "\n",
    "```python\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "```\n",
    "\n",
    "The *hyperparameter* ```learning_rate``` sets the step size to take\n",
    "for each iteration down the hill.\n",
    "\n",
    "//img src='https://cs231n.github.io/assets/nn3/opt1.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```grad``` calculates gradients and loss value.  By iteratively\n",
    "calculating the loss and gradient for each batch, we'll adjust the model\n",
    "during training.  \n",
    "Gradually, the model will find the best combination of weights and bias\n",
    "to minimize loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "loss_value, gradients = grad(model, features, labels)\n",
    "\n",
    "# Print the initial loss, no optimisation\n",
    "print(f\"Step: {optimizer.iterations.numpy()} \\\n",
    "           \\nInitial Loss: {loss_value.numpy()}\")\n",
    "\n",
    "\n",
    "# Calculate a single optimisation step\n",
    "optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "print(f\"Step:  {optimizer.iterations.numpy()} \\\n",
    "           \\nLoss: {loss(model, features, labels).numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop\n",
    "The model is ready for training.  \n",
    "A *training loop* feeds the dataset examples into the ```model```\n",
    "to help it make better predictions.  \n",
    "\n",
    "The following code sets up these *training steps*:\n",
    "1. Iterate each **epoch**. An epoch is one pass through the dataset.\n",
    "2. Within an epoch, iterate over each example in the training ```Dataset```\n",
    "grabbing its ```features``` and (actual)```label```.\n",
    "3. Using the example features, make a prediction and compare it\n",
    "with the label. Measure the inaccuracy and use that to calculate the gradients\n",
    "(via ```grad``` internal call to ```loss```).\n",
    "4. Use an ```optimizer``` to update the model variables.\n",
    "5. Keep track of stats for later visualization.\n",
    "\n",
    "```num_epochs``` is the number of times to loop over the dataset.\n",
    "> Counter-intuitively, training a model longer does not\n",
    "guarantee a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Rerunning this cell uses the same model variables\n",
    "# Keep results for plotting\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "num_epochs = 201\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "    # Training loop - using batches of 32\n",
    "    for x, y in train_dataset:\n",
    "        # Optimize the model\n",
    "        loss_value, grads = grad(model, x, y)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Track progress\n",
    "        # for each batch\n",
    "        # Add current batch loss \n",
    "        epoch_loss_avg(loss_value)\n",
    "        # Compare predicted label to actual label\n",
    "        epoch_accuracy(y, model(x))\n",
    "\n",
    "    # At end epoch append average loss and epoch accuracy\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "    train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "    # Print results at set number of epochs\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch: {epoch:03d} \\\n",
    "            Loss: {epoch_loss_avg.result():.3f} \\\n",
    "            Accuracy: {epoch_accuracy.result():.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise the loss function over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
    "fig.suptitle('Training Metrics')\n",
    "\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "axes[0].plot(train_loss_results)\n",
    "\n",
    "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
    "axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
    "axes[1].plot(train_accuracy_results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Evaluate the model on the test set\n",
    "Load the test data in a ```tf.dataset```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    test_fp,\n",
    "    batch_size,\n",
    "    column_names=column_names,\n",
    "    label_name='species',\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "\n",
    "test_dataset = test_dataset.map(pack_features_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model evaluates only a single epoch of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = tf.keras.metrics.Accuracy()\n",
    "\n",
    "for (x, y) in test_dataset:\n",
    "    logits = model(x)\n",
    "    prediction = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "    test_accuracy(prediction, y)\n",
    "\n",
    "print(f\"Test set accuracy: {test_accuracy.result():.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the model to make predictions\n",
    "The trained model can be used to make predictions on *unlabeled examples*,\n",
    "i.e examples that contain a feature but no label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dataset = tf.convert_to_tensor([\n",
    "    [5.1, 3.3, 1.7, 0.5],\n",
    "    [5.9, 3.0, 4.2, 1.5],\n",
    "    [6.9, 3.1, 5.4, 2.1]\n",
    "])\n",
    "\n",
    "predictions = model(predict_dataset)\n",
    "\n",
    "for i, logits in enumerate(predictions):\n",
    "    class_idx = tf.argmax(logits).numpy()\n",
    "    p = tf.nn.softmax(logits)[class_idx]\n",
    "    name = class_names[class_idx]\n",
    "    print(f\"Example {i} prediction: {name} ({100*p:4.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now set up our machine learning model as a web service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## II - Deploy the classifier as a service\n",
    "Expose Jupyter cells as REST endpoints for front-end consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "iris_test_data = pd.read_csv(test_fp,\n",
    "                             skiprows=[0],\n",
    "                             names=column_names\n",
    "                             )\n",
    "\n",
    "\n",
    "# Encode label classes to string names with np.where(condition, [x, y])\n",
    "iris_test_data['species'] = np.where(iris_test_data['species'] == 0,\n",
    "                                     'setosa',\n",
    "                                     np.where(iris_test_data['species'] == 1,\n",
    "                                              'versicolor',\n",
    "                                              'virginica'\n",
    "                                              ))\n",
    "\n",
    "iris_test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let’s create a function that returns a random Iris flower from the test dataset in our notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_iris():\n",
    "    \"\"\"\n",
    "    Choose a random flowe from 3 possible choices.\n",
    "    Selects the url for that choice and retrieves the data\n",
    "    associated with the flower.\n",
    "    \"\"\"\n",
    "    choices = ['setosa', 'versicolor', 'virginica']\n",
    "    random_flower = random.choice(choices)\n",
    "    flowers = {\n",
    "        'setosa': 'https://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Kosaciec_szczecinkowaty_Iris_setosa.jpg/440px-Kosaciec_szczecinkowaty_Iris_setosa.jpg',\n",
    "        'versicolor': 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Iris_versicolor_3.jpg/440px-Iris_versicolor_3.jpg',\n",
    "        'virginica': 'https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Iris_virginica.jpg/440px-Iris_virginica.jpg'\n",
    "    }\n",
    "    random_url = flowers[random_flower]\n",
    "    single_type = iris_test_data[(iris_test_data['species'] == random_flower)]\n",
    "    random_row = single_type.sample(n=1).drop(['species'], axis=1)\n",
    "    final = random_flower, random_url, random_row\n",
    "    return(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will call this function using an endpoint so our application can fetch a random flower name, its image URL and features data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Kernel Gateway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To turn ```show_random_iris``` into an endpoint we use the Kernel Gateway by wrapping the following around our code:\n",
    "\n",
    "* a GET comment on the first line of the cell;\n",
    "* a print command that outputs JSON.\n",
    "\n",
    "This will make making the function callable from our web application in the browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET /get_random_flower\n",
    "\n",
    "random_result = show_random_iris()\n",
    "res = {\n",
    "    \"name\" : random_result[0],\n",
    "    \"url\" : random_result[1],\n",
    "    \"data\" : {\"keys\" : list(random_result[2]), \n",
    "              \"values\" : random_result[2].iloc[0].tolist()}\n",
    "}\n",
    "\n",
    "print(json.dumps(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terminal use this command in the *same directory* of flower_app.ipynb to check out the enpoint.\n",
    "\n",
    "```python\n",
    "jupyter kernelgateway --KernelGatewayApp.api='kernel_gateway.notebook_http' --KernelGatewayApp.ip=0.0.0.0 --KernelGatewayApp.port=9090 --KernelGatewayApp.seed_uri=flower_app.ipynb --KernelGatewayApp.allow_origin='*'\n",
    "```\n",
    "\n",
    "The command does the following:\n",
    "\n",
    "* starts Jupyter gateway;\n",
    "* sets the base URL to localhost (0.0.0.0);\n",
    "* exposes port 9090 to browsers (you can change this to any available port);\n",
    "* assigns the flower_power.ipynb notebook to the gateway;\n",
    "* allows incoming traffic from everywhere (remember, we’re just prototyping)\n",
    "\n",
    "Open a new tab and point to **localhost:9090/get_random_flower** to get the data returned.\n",
    "\n",
    "---\n",
    "If the below error appears, fix the code commenting out the line as below.\n",
    "AttributeError: module 'signal' has no attribute 'SIGHUP'\n",
    "https://github.com/hyperopt/hyperopt/issues/332\n",
    "\n",
    "```python\n",
    "def start(self):\n",
    "    \"\"\"Starts an IO loop for the application.\"\"\"\n",
    "    super(KernelGatewayApp, self).start()\n",
    "    self.log.info(\n",
    "        'Jupyter Kernel Gateway at http{}://{}:{}'.format(\n",
    "            's' if self.keyfile else '', self.ip, self.port))\n",
    "    self.io_loop = ioloop.IOLoop.current()\n",
    "\n",
    "    # signal.signal(signal.SIGHUP, signal.SIG_IGN)\n",
    "\n",
    "    signal.signal(signal.SIGTERM, self._signal_stop)\n",
    "\n",
    "    try:\n",
    "        self.io_loop.start()\n",
    "    except KeyboardInterrupt:\n",
    "        self.log.info(\"Interrupted...\")\n",
    "    finally:\n",
    "        self.shutdown()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Swagger to check the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter’s Kernel Gateway automatically generates a swagger.json endpoint we can access using SwaggerUI.\n",
    "\n",
    "Download and install docker and pull Swagger from terminal\n",
    "\n",
    "```\n",
    "docker pull swaggerapi/swagger-ui\n",
    "```\n",
    "\n",
    "Type docker images in the console to see the name of the image. We can usa that name to run its container\n",
    "\n",
    "```\n",
    "docker run -p 80:8080 swaggerapi/swagger-ui\n",
    "```\n",
    "\n",
    "Open swagger-ui in our browser on port 80\n",
    "\n",
    "```\n",
    "http://localhost:80\n",
    "```\n",
    "\n",
    "Add ```http://localhost:9090/_api/spec/swagger.json``` in the explore bar to try out the GET response from the endpoint.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up other endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_flower(guess_string):\n",
    "    guess_list = eval(guess_string)\n",
    "    predict_dataset = tf.convert_to_tensor([guess_list])\n",
    "    predictions = model(predict_dataset)\n",
    "    for i, logits in enumerate(predictions):\n",
    "        class_idx = tf.argmax(logits).numpy()\n",
    "        name = class_ids[class_idx]\n",
    "    return(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET /predict_flower\n",
    "\n",
    "req = json.loads(REQUEST)\n",
    "args = req['args']\n",
    "\n",
    "guess_string = args['guess_string'][0] # first argument from URL \n",
    "\n",
    "predicted_flower = predict_flower(guess_string)\n",
    "\n",
    "print(json.dumps(predicted_flower))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to start prototyping a real application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## III - App prototype setup with Azle\n",
    "\n",
    "Azle is a front-end prototyping library designed specifically for rapidly crafting applications. \n",
    "\n",
    "It uses functions to add and style content, bind events, and interact with API endpoints, making it easier to build full-powered, data-driven applications.\n",
    "\n",
    "Start by creating an html template creating a index.html file i nthe same directory.\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "    \n",
    "<head>\n",
    "<script src='https://azlejs.com/v2/azle.min.js'></script>\n",
    "</head>\n",
    "    \n",
    "<body>\n",
    "</body>\n",
    "</html>\n",
    "    \n",
    "<script> \n",
    " \n",
    "create_azle(function() {\n",
    "\n",
    "// YOUR AZLE FUNCTIONS GO HERE\n",
    "\n",
    "})\n",
    "\n",
    "</script>\n",
    "```\n",
    "\n",
    "All our Azle functions can be added between the ```<script>``` tags at the bottom. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding content\n",
    "\n",
    "Each function uses a target_class and target_instance as the first 2 parameters. This tells Azle where to place the content, or which content we wish to style.\n",
    "\n",
    "The 3rd parameter is an object containing properties we wish to pass. In our first slider example we passed in a class name (“this_class”) along with default values. \n",
    "\n",
    "For example, create a section and add a slider to the section with default, min and max values:\n",
    "\n",
    "```javascript\n",
    "az.add_sections({\n",
    "    \"this_class\": \"my_sections\",\n",
    "    \"sections\": 1\n",
    "})\n",
    "   \n",
    "    \n",
    "az.add_slider({\"my_sections\", 1, {\n",
    "    \"this_class\": \"my_slider\",\n",
    "    \"default_value\": 4,\n",
    "    \"min_value\": 0,\n",
    "    \"max_value\": 10\n",
    "    })\n",
    "})    \n",
    "```\n",
    "\n",
    "Other elements\n",
    "\n",
    "```javascript\n",
    "az.add_button(target_class, target_instance, {\n",
    "    \"this_class\" : \"my_button\",\n",
    "    \"text\" : \"CLICK ME\"\n",
    "})\n",
    "\n",
    "    \n",
    "az.add_dropdown(target_class, target_instance, {\n",
    "    \"this_class\" : \"my_dropdown\",\n",
    "    \"options\" : [\"option A\", \n",
    "                 \"option B\", \n",
    "                 \"option C\", \n",
    "                 \"option D\"],\n",
    "    \"title\" : \"choose option...\"\n",
    "})\n",
    "\n",
    "\n",
    "az.add_slider(\"target_class\", 1, {\n",
    "    \"this_class\" : \"my_slider\",\n",
    "    \"default_value\" : 4,\n",
    "    \"min_value\" : 0,\n",
    "    \"max_value\" : 10\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Styling content\n",
    "\n",
    "Styling is done using target_class and target instance as above, and passing style properties into a style object as the 3rd parameter:\n",
    "\n",
    "```javascript\n",
    "az.style_button('my_button', 1, {\n",
    "    \"background\" : \"hotpink\",\n",
    "    \"border-radius\" : \"6px\",\n",
    "    \"width\" : \"200px\",\n",
    "    \"color\" : \"black\",\n",
    "    \"cursor\" : \"wait\"\n",
    "})   \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding events\n",
    "\n",
    "Bind events to our UI elements using the add_event function.\n",
    "\n",
    "```javascript\n",
    "az.add_event(\"my_button\", 1, {\n",
    "    \"type\" : \"click\",\n",
    "    \"function\" : \"alert('you clicked me')\"\n",
    "})  \n",
    "```\n",
    "\n",
    "To see all available events run ththis in [jsfiddle](https://jsfiddle.net/)\n",
    "\n",
    "```javascript\n",
    "create_azle(function() {\n",
    "\n",
    "  az.add_sections({\n",
    "      \"this_class\" : \"my_sections\", \n",
    "      \"sections\" : 1\n",
    "  })\n",
    "  \n",
    "  az.add_button(\"my_sections\", 1, {\n",
    "      \"this_class\" : \"show_icons\", \n",
    "      \"text\" : \"SHOW EVENTS\"\n",
    "  })\n",
    "  \n",
    "  az.add_event('show_icons', 1, {\n",
    "      \"type\" : \"click\", \n",
    "      \"function\" : \"az.show_events()\"\n",
    "  })\n",
    "\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling endpoints\n",
    "\n",
    "Calling endpoints is how we send and fetch data from REST APIs. Azle’s call_api() function asks for the base url, any parameters we need to pass, and the function to call once the response has been received:\n",
    "\n",
    "```javascript\n",
    "// list any parameters required by the API endpoint\n",
    "params = {\n",
    "    \"postId\" : 6\n",
    "}\n",
    "\n",
    "az.call_api({\n",
    "    \"url\" : \"https://jsonplaceholder.typicode.com/comments\",\n",
    "    \"parameters\" : params,\n",
    "    \"done\" : \"alert(JSON.stringify(data))\"\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Create the app\n",
    "\n",
    "App folder structure:\n",
    "\n",
    "```\n",
    "flower_app\n",
    "├── scripts\n",
    "├── d3_visuals\n",
    "├── flower_power.ipynb\n",
    "├── index.html\n",
    "```\n",
    "\n",
    "We will need to view our application in another browser tab, meaning we need to “spin up a web server.”:\n",
    "\n",
    "```\n",
    "python -m http.server\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
